prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
pen_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- pen_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
library(tidyverse)
library(tidymodels)
library(patchwork)
library(vroom)
library(DataExplorer)
library(ggplot2)
library(timetk)
library(dplyr)
library(glmnet)
library(dials)
setwd("C://Users//Isaac//OneDrive//Documents//fall 2025 semester//STAT 348//BiekShare")
train <- vroom("train.csv")
test <- vroom("test.csv")
train_ud <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_rm(datetime)%>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
my_mod <- rand_forest(mtry = tune(),
min_n=tune(),
trees = 500) %>%
set_engine("ranger") %>%
set_mode("regression")
pen_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_mod)
#
my_grid <- grid_regular(mtry(range=c(1, ncol(prepped_recipe) - 1)),
min_n(),
levels=5)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(vroom)
library(DataExplorer)
library(ggplot2)
library(timetk)
library(dplyr)
library(glmnet)
library(dials)
setwd("C://Users//Isaac//OneDrive//Documents//fall 2025 semester//STAT 348//BiekShare")
train <- vroom("train.csv")
test <- vroom("test.csv")
train_ud <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_rm(datetime)%>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
pen_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- pen_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
#
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
library(tidyverse)
library(tidymodels)
library(patchwork)
library(vroom)
library(DataExplorer)
library(ggplot2)
library(timetk)
library(dplyr)
library(glmnet)
setwd("C://Users//Isaac//OneDrive//Documents//fall 2025 semester//STAT 348//BiekShare")
train <- vroom("train.csv")
test <- vroom("test.csv")
train_ud <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_rm(datetime)%>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
pen_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- pen_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
#
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
?dbinom
dbinom(3, size = 10, prob = .2)
dbinom(2:4, size = 10, prob = .2)
dbinom(0:10, size = 10, prob = .2)
dbinom(10, size = 10, prob = .9)
x <- 10
pmf <- factorial(10) / (factorial(x) * factorial(10 - x)) *
(0.9^x) * (0.1^(10 - x))
plot(x, pmf, type = "h", lwd = 5, col = "blue",
xlab = "x", ylab = "Probability",
main = "Binomial PMF")
pmf <- factorial(10) / (factorial(x) * factorial(10 - x)) *
(0.9^x) * (0.1^(10 - x))
plot(x, pmf, type = "h", lwd = 5, col = "blue",
xlab = "x", ylab = "Probability",
main = "Binomial PMF")
pmf <- dbinom(0:10, size = 10, prob = .9)
plot(pmf, type = "h", lwd = 3, col = "purple",
x <- 0:10
n <- 10
prob = 0.9
x <- 0:10,
x <- 0:10
x <- 0:10
n <- 10
prob <- 0.9
pmf <- dbinom(x, size = n, prob)
plot(pmf, type = "h", lwd = 3, col = "mediumpurple4",
main = "Binomial PMF)
x <- 0:10
n <- 10
prob <- 0.9
pmf <- dbinom(x, size = n, prob)
plot(pmf, type = "h", lwd = 3, col = "mediumpurple4",
plot(pmf, lwd = 3, col = "mediumpurple4",
plot(pmf)
library(ggplot2)
library(ggplot2)
pmf <- dbinom(x, size = n, prob = prob)
plot(pmf)
plot(pmf, type = "h")
plot(pmf, type = "h", lwd = 2, col = "mediumpurple")
plot(pmf, type = "h", lwd = 2, col = "mediumpurple")
plot(pmf, type = "h", lwd = 5, col = "mediumpurple")
plot(pmf, type = "h", lwd = 10, col = "mediumpurple")
x <- 0:10
n <- 10
prob <- 0.85
pmf <- dbinom(x, size = n, prob = prob)
plot(pmf, type = "h", lwd = 10, col = "mediumpurple")
x <- 0:10
n <- 10
prob <- 0.85
pmf <- dbinom(x, size = n, prob = prob)
plot(pmf, type = "h", lwd = 10, col = "mediumpurple")
library(tidyverse)
library(tidymodels)
library(patchwork)
library(vroom)
library(DataExplorer)
library(ggplot2)
library(timetk)
library(dplyr)
library(glmnet)
library(dials)
setwd("C://Users//Isaac//OneDrive//Documents//fall 2025 semester//STAT 348//BiekShare")
train <- vroom("train.csv")
test <- vroom("test.csv")
train_ud <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_rm(datetime)%>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
pen_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth,
cost_complexity(),
min_n(),
levels = 10)
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_rm(datetime)%>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
tree_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- tree_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(timestamp, features="dow") %>%
step_time(timestamp, features=c("hour", "minute")) %>%
step_rm(datetime)%>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(timestamp, features="dow") %>%
step_time(timestamp, features=c("hour", "minute")) %>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
train_ud <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(timestamp, features="dow") %>%
step_time(timestamp, features=c("hour", "minute")) %>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(datetime, features="dow") %>%
step_time(datetime, features=c("hour", "minute")) %>%
step_corr(all_numeric_predictors(), threshold=0.5) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
tree_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- tree_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(datetime, features="dow") %>%
step_time(datetime, features=c("hour", "minute")) %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(datetime, features="dow") %>%
step_time(datetime, features="hour") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
tree_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- tree_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
train_ud <- train |>
select(-casual, -registered) |>
mutate(count = log(count))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(datetime, features="dow") %>%
step_time(datetime, features="hour") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
tree_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- tree_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
my_recipe <- recipe(count ~., data=train_ud) %>%
step_mutate(weather= factor(ifelse(weather == "4", "3", weather),
levels=c("1","2","3"),
labels=c("Clear", "Mist", "Light Snow"))) %>%
step_mutate(season = factor(season, levels=c("1","2","3","4"),
labels=c("Spring","Summer","Fall","Winter"))) %>%
step_date(datetime, features="dow") %>%
step_time(datetime, features="hour") %>%
step_dummy(all_nominal_predictors()) %>%
step_normalize(all_numeric_predictors())
prepped_recipe <- prep(my_recipe) # Sets up the preprocessing using myDataSet13
bake(prepped_recipe, new_data=train_ud)
tree_mod <- decision_tree(tree_depth = tune(),
cost_complexity = tune(),
min_n=tune()) %>%
set_engine("rpart") %>%
set_mode("regression")
head(train_ud, 5)
tree_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(tree_mod)
#
grid_of_tuning <- grid_regular(tree_depth(),
cost_complexity(),
min_n(),
levels = 10)
#
folds <- vfold_cv(train_ud, v = 10, repeats=1)
#
CV_results <- tree_workflow %>%
tune_grid(resamples=folds,
grid=grid_of_tuning,
metrics=metric_set(rmse, mae))
#
collect_metrics(CV_results) %>% # Gathers metrics into DF
filter(.metric=="rmse") %>%
ggplot(data=., aes(x=penalty, y=mean, color=factor(mixture))) +
geom_line()
#
bestTune <- CV_results %>%
select_best(metric="rmse")
final_wf <-
pen_workflow %>%
finalize_workflow(bestTune) %>%
fit(data=train_ud)
final_wf %>%
predict(new_data = test)
## Run all the steps on test data15
pen_predictions <- predict(final_wf, new_data = test)
pen_predictions
kaggle_submission <- pen_predictions %>%
bind_cols(., test) %>%
select(datetime, .pred) %>%
rename(count=.pred) %>%
mutate(count=pmax(0, count)) %>%
mutate(count = exp(count)) %>%
mutate(datetime=as.character(format(datetime)))
vroom_write(x=kaggle_submission, file="./LinearPreds.csv", delim=",")
